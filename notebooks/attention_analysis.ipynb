{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5296e558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniil/Learn/Semestr_3/Modern_CV/Fine-tuning_1.2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from src.attention_analysis import AttentionAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eff4e57",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for '../models/vit-fashion-mnist-fast'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../models/vit-fashion-mnist-fast' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning_1.2/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:354\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m     resolved_image_processor_files = [\n\u001b[32m    334\u001b[39m         resolved_file\n\u001b[32m    335\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    353\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     resolved_image_processor_file = \u001b[43mresolved_image_processor_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Инициализация анализатора\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m analyzer = \u001b[43mAttentionAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../models/vit-fashion-mnist-fast\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning_1.2/notebooks/../src/attention_analysis.py:20\u001b[39m, in \u001b[36mAttentionAnalyzer.__init__\u001b[39m\u001b[34m(self, model_path, device)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.model = ViTModel.from_pretrained(\n\u001b[32m     14\u001b[39m     model_path,\n\u001b[32m     15\u001b[39m     output_attentions=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# ВАЖНО: включаем attention\u001b[39;00m\n\u001b[32m     16\u001b[39m     add_pooling_layer=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ).to(device)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mself\u001b[39m.processor = \u001b[43mViTImageProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.patch_size = \u001b[38;5;28mself\u001b[39m.model.config.patch_size\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.num_heads = \u001b[38;5;28mself\u001b[39m.model.config.num_attention_heads\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning_1.2/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:201\u001b[39m, in \u001b[36mImageProcessingMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m image_processor_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_dict(image_processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learn/Semestr_3/Modern_CV/Fine-tuning_1.2/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:361\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    360\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    362\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load image processor for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m it from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m         )\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[31mOSError\u001b[39m: Can't load image processor for '../models/vit-fashion-mnist-fast'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../models/vit-fashion-mnist-fast' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "# Инициализация анализатора\n",
    "analyzer = AttentionAnalyzer('../models/vit-fashion-mnist-fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка тестовых изображений\n",
    "dataset = load_dataset(\"fashion_mnist\")\n",
    "test_images = dataset['test']['image'][:10]  # Первые 10 изображений\n",
    "test_labels = dataset['test']['label'][:10]\n",
    "\n",
    "label_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a48508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ для первого изображения\n",
    "image_idx = 0\n",
    "image = test_images[image_idx]\n",
    "true_label = label_names[test_labels[image_idx]]\n",
    "\n",
    "print(f\"Анализ изображения: {true_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc766ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация для разных слоев\n",
    "for layer in [0, 4, 8]:\n",
    "    fig = analyzer.visualize_cls_attention(image, layer_idx=layer, head_idx=0)\n",
    "    fig.suptitle(f'Layer {layer} - Head 0 - {true_label}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983047d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Rollout для нескольких изображений\n",
    "for i in range(3):\n",
    "    image = test_images[i]\n",
    "    true_label = label_names[test_labels[i]]\n",
    "    \n",
    "    fig = analyzer.attention_rollout(image)\n",
    "    fig.suptitle(f'Attention Rollout - {true_label}', fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравнение attention по слоям для одного изображения\n",
    "analyzer.compare_heads_layers(\n",
    "    test_images[0], \n",
    "    save_path='../reports/attention_visualizations/layers_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ разнообразия attention heads\n",
    "diversity_scores = analyzer.analyze_head_diversity(test_images[0])\n",
    "\n",
    "print(\"Разнообразие attention heads по слоям:\")\n",
    "for layer, diversity in diversity_scores:\n",
    "    print(f\"Layer {layer}: {diversity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "layers = [score[0] for score in diversity_scores]\n",
    "diversities = [score[1] for score in diversity_scores]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(layers, diversities, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Diversity Score')\n",
    "plt.title('Diversity of Attention Heads Across Layers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../reports/attention_visualizations/head_diversity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399042ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование с разными размерами изображений\n",
    "def test_different_resolutions():\n",
    "    original_image = test_images[0]\n",
    "    \n",
    "    # Разные размеры\n",
    "    sizes = [(56, 56), (112, 112), (224, 224)]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(sizes), figsize=(15, 5))\n",
    "    \n",
    "    for idx, size in enumerate(sizes):\n",
    "        # Ресайз изображения\n",
    "        resized_image = original_image.resize(size)\n",
    "        \n",
    "        # Анализ внимания\n",
    "        attentions, _, _ = analyzer.get_attention_maps(resized_image)\n",
    "        cls_attention = attentions[-1][0].mean(dim=0)[0, 1:].cpu().numpy()\n",
    "        \n",
    "        grid_size = int(np.sqrt(cls_attention.shape[0]))\n",
    "        attention_map = cls_attention.reshape(grid_size, grid_size)\n",
    "        \n",
    "        axes[idx].imshow(attention_map, cmap='hot')\n",
    "        axes[idx].set_title(f'Size: {size}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/attention_visualizations/resolution_impact.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "test_different_resolutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e17f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ attention для разных классов\n",
    "class_analysis = {}\n",
    "\n",
    "for class_id in range(5):  # Первые 5 классов\n",
    "    class_images = [img for img, lbl in zip(test_images, test_labels) if lbl == class_id]\n",
    "    \n",
    "    if class_images:\n",
    "        # Анализ для первого изображения класса\n",
    "        attention_maps = []\n",
    "        for layer in [0, 6, 11]:  # Начальный, средний, конечный слои\n",
    "            fig = analyzer.visualize_cls_attention(\n",
    "                class_images[0], \n",
    "                layer_idx=layer, \n",
    "                head_idx=0\n",
    "            )\n",
    "            attention_maps.append(fig)\n",
    "        \n",
    "        class_analysis[label_names[class_id]] = attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количественные метрики внимания\n",
    "def calculate_attention_metrics(attention_maps):\n",
    "    \"\"\"Вычисление метрик внимания\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Focus score (насколько внимание сконцентрировано)\n",
    "    entropy = -torch.sum(attention_maps * torch.log(attention_maps + 1e-8), dim=-1)\n",
    "    metrics['focus_score'] = 1 - (entropy / np.log(attention_maps.shape[-1]))\n",
    "    \n",
    "    # Coverage (сколько патчей охвачено вниманием)\n",
    "    coverage = (attention_maps > attention_maps.mean()).float().mean(dim=-1)\n",
    "    metrics['coverage'] = coverage\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение к тестовым изображениям\n",
    "for i in range(3):\n",
    "    attentions, _, _ = analyzer.get_attention_maps(test_images[i])\n",
    "    \n",
    "    # Анализ последнего слоя\n",
    "    last_layer_attention = attentions[-1][0].mean(dim=0)  # Усредняем по головам\n",
    "    cls_attention = last_layer_attention[0, 1:]\n",
    "    \n",
    "    metrics = calculate_attention_metrics(cls_attention.unsqueeze(0))\n",
    "    \n",
    "    print(f\"Image {i} ({label_names[test_labels[i]]}):\")\n",
    "    print(f\"  Focus Score: {metrics['focus_score'].mean().item():.3f}\")\n",
    "    print(f\"  Coverage: {metrics['coverage'].mean().item():.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
